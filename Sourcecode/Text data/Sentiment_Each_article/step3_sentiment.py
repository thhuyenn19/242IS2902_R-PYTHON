import pandas as pd
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ‚úÖ Load model v√† tokenizer t·ª´ Hugging Face
model_name = "mr4/phobert-base-vi-sentiment-analysis"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

df = pd.read_csv("data2.csv")
batch_size = 128
texts = df["processed_title"].astype(str).tolist()
sentiments = []  # Danh s√°ch l∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n c·∫£m x√∫c


for i in range(0, len(texts), batch_size):
    batch_texts = texts[i : i + batch_size]  # Chia batch
    inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=256).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    labels = ["negative", "neutral", "positive"]
    batch_sentiments = [labels[i] for i in torch.argmax(probs, dim=-1).tolist()]
    sentiments.extend(batch_sentiments)
    df.loc[i:i + batch_size - 1, "sentiment"] = batch_sentiments
    df.to_csv("news_with_sentiment_temp.csv", index=False)  # L∆∞u t·∫°m th·ªùi
    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {min(i+batch_size, len(texts))}/{len(texts)} d√≤ng...")

# ‚úÖ L∆∞u file k·∫øt qu·∫£ cu·ªëi c√πng
df.to_csv("vietstock_sen.csv", index=False)

print("üéâ Ho√†n th√†nh! File k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'news_with_sentiment.csv'.")
